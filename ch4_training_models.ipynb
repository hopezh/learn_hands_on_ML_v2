{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Models\n",
    "\n",
    "* linear regression Models\n",
    "    * direct \"closed-form\" equation\n",
    "    * Gradient Descent (GD)\n",
    "\n",
    "* polynomial regression (for nonlinear datasets)\n",
    "    * overfitting\n",
    "    * learning curves\n",
    "    * regularization\n",
    "\n",
    "* logistic regression\n",
    "\n",
    "* softmax regression"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "a linear model makes a prediction by computing a weighted sum of the input features, plus a constant called the bias term (intercept term)\n",
    "\n",
    "* (linear regression model prediction) ------------------ : y = zeta_0 + zeta_1 * x_1 + zeta_2 * x_2 + zeta_n * x_n \n",
    "* (linear regression model prediction in vectorized form) : y = h_zeta(x) = zeta * x\n",
    "\n",
    "    * **_zeta_** is the model's **_parameter vector_**, including the bias term zeta_0 and the feature weightes zeta_1 to zeta_n\n",
    "    * **_x_** is the instance's **_feature vector_**, including x_0 to x_n with x_0=1 \n",
    "    * **_zeta * x_** is the **_dot product_** of the two vectors\n",
    "    * **_h_zeta_** is the **_hypothesis function_**, using the model parameters zeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}